\documentclass[portrait,final,archD,fontscale=0.477]{baposter}
% Currently set to 24" wide by 36" tall (archD).
% See baposter.cls for info on changing paper dimensions

\usepackage{calc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{relsize}
\usepackage{multirow}
\usepackage{rotating}
\usepackage{bm}
\usepackage{url}

\usepackage{graphicx}
\usepackage{multicol}

%\usepackage{times}
%\usepackage{helvet}
%\usepackage{bookman}
\usepackage{palatino}
%\usepackage{fourier}
%\usepackage{avant}

\input{math_commands.tex}

\newcommand{\captionfont}{\footnotesize}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% Some math symbols used in the text
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Multicol Settings
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\columnsep}{1.5em}
\setlength{\columnseprule}{0mm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Save space in lists. Use this after the opening of the list
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\compresslist}{%
\setlength{\itemsep}{1pt}%
\setlength{\parskip}{0pt}%
\setlength{\parsep}{0pt}%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Begin of Document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Here starts the poster
%%%---------------------------------------------------------------------------
%%% Format it to your taste with the options
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Define some colors

\definecolor{stevensred}{rgb}{0.6392,0.149,0.2196}
\definecolor{stevensgray}{rgb}{0.60392, 0.596, 0.60392}
\definecolor{ttblue}{rgb}{0.1568, 0.2, 0.349}

%%
\begin{poster}%
  % Poster Options
  {
  % Show grid to help with alignment
  grid=false,
  % Column spacing
  colspacing=1em,
  % Color style
  bgColorOne=white,
  bgColorTwo=white,
  borderColor=stevensgray,
  headerColorOne=ttblue,
  headerColorTwo=ttblue,
  headerFontColor=white,
  boxColorOne=white,
  boxColorTwo=stevensgray,
  % Format of textbox
  textborder=rectangle,
  % Format of text header
  eyecatcher=true,
  headerborder=closed,
  headerheight=0.1\textheight,
%  textfont=\sc, An example of changing the text font
  headershape=rectangle,
  headershade=shadelr,
  headerfont=\Large\bf\textsc, %Sans Serif
  textfont={\setlength{\parindent}{1.5em}},
  boxshade=plain,
%  background=shade-tb,
  background=plain,
  linewidth=2pt
  }
  % Eye Catcher
  {% The makebox allows the title to flow into the logo, this is a hack because of the L shaped logo.
    \includegraphics[height=10.0em]{img/tokyotech_logo}
  }
  % Title
  {\bf \Huge \textsc{Learning GNNs with Noisy Labels} }
  % Authors
  {\vspace{0.5em} \textsc{ Hoang NT, Jun Jin Choong, Tsuyoshi Murata \\ \vspace{0.2em} Tokyo Institute of Technology}}
  % University logo
  {% The makebox allows the title to flow into the logo, this is a hack because of the L shaped logo.
    \includegraphics[height=8.0em]{img/lld_logo}
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Now define the boxes that make up the poster
%%%---------------------------------------------------------------------------
%%% Each box has a name and can be placed absolutely or relatively.
%%% The only inconvenience is that you can only specify a relative position 
%%% towards an already declared box. So if you have a box attached to the 
%%% bottom, one to the top and a third one which should be in between, you 
%%% have to specify the top and bottom boxes before you specify the middle 
%%% box.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    %
    % A coloured circle useful as a bullet with an adjustably strong filling
    \newcommand{\colouredcircle}{%
      \tikz{\useasboundingbox (-0.2em,-0.32em) rectangle(0.2em,0.32em); \draw[draw=black,fill=lightblue,line width=0.03em] (0,0) circle(0.18em);}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \headerbox{Introduction}{name=problem,column=0,row=0}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
We study the robustness to symmetric label noise of GNNs 
training procedures. By combining the nonlinear neural 
message-passing models (e.g. Graph Isomorphism Networks, 
GraphSAGE, etc.) with loss correction methods, we present a
noise-tolerant approach for the graph classification task. 
Our experiments show that test accuracy can be improved 
under the artificial symmetric noisy setting.

\vspace{0.5em}
\includegraphics[width=0.9\linewidth]{img/scheme}
%\includegraphics[width=0.8\linewidth]{img/_noisy_training}
\vspace{0.5em}
\includegraphics[width=\linewidth]{img/MUTAG_noisy_training}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Short References}{name=references,column=0,above=bottom}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
    \smaller
    \bibliographystyle{ieee}
    \renewcommand{\section}[2]{\vskip 0.05em}
    \begin{thebibliography}{1}\itemsep=-0.01em
      \setlength{\baselineskip}{0.4em}

      \bibitem{patrini2017making} Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Q. "Making deep neural networks robust to label noise: A loss correction approach," \textit{IEEE Conference on Computer Vision and Pattern Recognition}, pp.
      1944–1952, 2017.

      \bibitem{xu2018how} Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. "How powerful are graph neural networks?," \textit{International Conference on Learning Representations}, ICLR 2019.

      \bibitem{hamilton2017inductive} Will Hamilton, Zhitao Ying, and Jure Leskovec. "Inductive representation learning on large graphs," \textit{Advances in Neural Information Processing Systems}, pp. 1024–1034, 2017.

      \bibitem{kipf2017semi} Thomas N. Kipf and Max Welling. "Semi-supervised classification with graph convolutional networks," \textit{International Conference on Learning Representations}, ICLR 2017.
   \end{thebibliography}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \headerbox{Graph Classification Model}{name=method,column=0,below=problem, above=references}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
We use a nonlinear message passing model similar to \emph{GIN} \cite{xu2018how} 
and \emph{GraphSAGE} \cite{hamilton2017inductive}. In the supervised learning
setting, such model has two main steps:

\begin{enumerate}
  \item Accumulate neighborhood information to each vertex $\vv$ and neural network layer $(k)$:
\vspace{-0.5em}
\begin{equation*}
  \begin{split}
\rva^{(k)}_{v} & = \texttt{AGGREGATE}^{(k)} (\{\rvh^{(k-1)}_u: u \in \mathcal{N}(v)\}), \\
\rvh^{(k)}_v & = \texttt{COMBINE}^{(k)} (\rvh_v^{(k-1)}, \rva_v^{(k)})
  \end{split}
\end{equation*}

  \item Employ a function to learn the overall representation of the graph, then the objective $\ell$ is optimized with standard backpropagation.
  \vspace{-0.5em}
\begin{equation*}
  \begin{split}
\rvh_{\gG} & = \texttt{READOUT} (\{\rvh^{(K)}_v: v \in \gG\}), \\
\ell(p(y|\rvh_\gG), y_\gG) & = \texttt{XENTROPY} (p(y|\rvh_\gG), y_\gG)
  \end{split}
\end{equation*}
\end{enumerate}

\noindent
$\rvh^{(k)}_v$: Feature vector of node $v$ at iteration $(k)$; 
$\rvh_\gG$: Feature vector of whole graph; $y_\gG$: True label 
of training graphs; $p(y|\rvh_\gG)$: A prediction model.
   

\hspace{-1em} \includegraphics[width=0.95\linewidth]{img/gnn}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\headerbox{Loss Correction Schemes}{name=results,column=1,span=2,row=0}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{multicols}{2}

\noindent
We employ the loss correction technique from the line 
of work in [3]. We propose to use the \emph{backward}
surrogate loss with different estimators $C$ for the noise matrix $N$:
$$\ell^{\leftarrow} = \mC^{-1} \cdot \ell(\hat{p}(y|\gG))$$
\noindent
Our final loss function for the GNN model is:
$$\ell^{\leftarrow}(p(y|\rvh_\gG), y_\gG) = \mC^{-1} \cdot  \texttt{CROSS\_ENTROPY} (p(y|\rvh_\gG), y_\gG)$$
Such loss correction can be understood as going backward one step in the noisy
process. To estimate $\mC$ from the noisy data (corrupted by some unknown 
noise process $\mN$), we propose two estimation schemes:

\begin{enumerate}
  \item Assume the neural network can exactly model the noisy data, hence the final softmax output is used to fill matrix $\mC$. For instance, out of all sample classified as class ``A'' in our training data, we find the one that gives the weakest response for class ``A'', and use its softmax output as the values for row ``A'' in matrix $\mC$.
  \item two
\end{enumerate}
\end{multicols}

\hspace{5em}
\begin{tabular}{l|ccccc}
\bf{Dataset} (\#classes) & diag($N$) & Avg. diag($\mC^{\text{c}}$) & $\norm{\mC^{\text{c}}-N}$ & Avg. diag($\mC^{\text{a}}$) & $\norm{\mC^{\text{a}}-N}$ \\
%\bf{Dataset} (\#classes) & diag($N$) & Avg. diag($\mC^{\text{c}}$) & & & \\
\hline \\
IMDB-B   (2) & 0.8  & 0.99   & 0.76  & 0.77 & 0.12\\
IMDB-M   (3) & 0.8  & 0.99   & 1.14  & 0.85 & 0.30\\
RDT-B    (2) & 0.8  & 0.99   & 0.76  & 0.75 & 0.20\\
RDT-M5K  (5) & 0.8  & 0.99   & 1.90  & 0.81 & 0.10\\
COLLAB   (3) & 0.8  & 0.99   & 1.14  & 0.75 & 0.30\\
MUTAG    (2) & 0.8  & 0.99   & 0.76  & 0.74 & 0.24\\
PROTEINS (2) & 0.8  & 0.99   & 0.76  & 0.78 & 0.08\\
PTC      (2) & 0.8  & 0.99   & 0.76  & 0.63 & 0.68\\
NCI1     (2) & 0.8  & 0.99   & 0.76  & 0.74 & 0.24\\
\end{tabular}
\vspace{1em}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  \headerbox{Experimental Results}{name=background model,column=1, span=2,below=results, above=bottom}{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{multicols}{2}

\hfill \\

{ 
\vspace{-1.5em}
\begin{tabular}{l|cccc}
\bf Dataset & \#graphs & \#classes & \#vertices \\
\hline \\
IMDB-B   & 1000 & 2 & 19.8  \\   
IMDB-M   & 1500 & 3 & 13.0  \\ 
RDT-B    & 2000 & 2 & 429.6 \\    
RDT-M5K  & 5000 & 5 & 508.5 \\       
COLLAB   & 5000 & 3 & 74.5  \\     
MUTAG    & 188  & 2 & 17.9  \\    
PROTEINS & 1113 & 2 & 39.1  \\       
PTC      & 344  & 2 & 25.5  \\  
NCI1     & 4110 & 2 & 29.8  \\   
\end{tabular}
}

\columnbreak

In this experiment, we simulate a robot keeping station, repeatedly scanning the same area. 

\textbf{Top}: Our method, BGKOctoMap updated with information from 1, 15, 30, and 60 scans (\textbf{left} to \textbf{right}) containing the same data.

\textbf{Bottom}: The result after applying the online Gaussian process occupancy mapping method in \cite{jwang} to the same data.

Qualitatively, our method (\textbf{top}) is more stable for long-term mapping scenarios with many repeat observations than the previous method (\textbf{bottom}) in which the occupied voxels tend to grow continuously with repeat observations.

\end{multicols}


\hspace{-1.5em}
  \begin{tabular}{lccccccccccc}
                                 & MUTAG   & IMDB-M    & RDT-B   & RDT-M5K & COLLAB    & IMDB-B   & PROTEINS    & PTC         & NCI1 \\
    \hline \\
    GIN                          & .7327   & .4476     & .6695   & .3677   & .6544     & .6573    & .6257       & .4824       & .6472 \\
    GraphSAGE                    & .7072   & .4373     &   -     & -       & -         & .6410    & .6583       & .4892       & .6053 
    \vspace{0.5em}\\
    \hline \vspace{-0.5em} \\
    D-GNN-C                      &  .5727  &  \bf.4747 & .5005   & .2000   &  .5979    & \bf.6940 & \bf.6693    & \bf.5557       &  .6170 \\
    D-GNN-A                      &  .7102  &  \bf.4505 & .5307   & .2000   &  \bf.6917 & \bf.7088 & \bf.6769    & \bf.5001       & .6405  \\
    D-GNN-E                      &  .7002  &  \bf.4633 & .5270   & .2022   &  \bf.6960 & \bf.7190 & \bf.6917    & \bf.5235       & \bf.6638 \\
  \end{tabular}

\includegraphics[width=0.95\linewidth]{img/notgood}

We also evaluated our method qualitatively on real data from the University of Freiburg, shown above. From \textbf{left} to \textbf{right} we have:

\textbf{1)} The raw range-sensor data from University of Freiburg Corridor FR-079

\textbf{2)} The map produced by standard OctoMap

\textbf{3)} The map produced by BGKOctoMap

\vspace{0.1em}

\setlength{\columnsep}{0.1em}
\vspace{0.1em}
\begin{multicols}{2}

\noindent
The source code is available at: \texttt{github.com/gear/denoising-gnn}.

\end{multicols}

}


\end{poster}

\end{document}

